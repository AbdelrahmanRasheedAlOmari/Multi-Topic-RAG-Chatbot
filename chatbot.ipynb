{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the topic folders\n",
    "TOPIC_FOLDERS = {\n",
    "    \"Python Programming\": \"Data/Python\",\n",
    "    \"Machine Learning_AI\": \"Data/AI_ML\",\n",
    "    \"UAE Information\": \"Data/UAE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Warning: Folder not found: {folder_path}\")\n",
    "        return documents\n",
    "        \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.startswith('.'): \n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if filename.endswith('.txt'):\n",
    "                loader = TextLoader(file_path)\n",
    "            elif filename.endswith('.pdf'):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {file_path}\")\n",
    "                continue\n",
    "            documents.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chains():\n",
    "    # Initialize embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Initialize chains dictionary\n",
    "    conversational_chains = {}\n",
    "    \n",
    "    # Process each topic\n",
    "    for topic, folder in TOPIC_FOLDERS.items():\n",
    "        print(f\"Processing {topic}...\")\n",
    "        \n",
    "        # Load and split documents\n",
    "        documents = load_documents(folder)\n",
    "        if not documents:\n",
    "            print(f\"No documents found for {topic}\")\n",
    "            continue\n",
    "            \n",
    "        split_docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=split_docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=topic.lower().replace(\" \", \"_\")\n",
    "        )\n",
    "        \n",
    "        # Initialize retriever and memory\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        # Create chain\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0.7),\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        conversational_chains[topic] = chain\n",
    "        print(f\"{topic}: Chain initialized\")\n",
    "    \n",
    "    return conversational_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_function(user_input, selected_topic, history):\n",
    "    if not selected_topic:\n",
    "        return history, history\n",
    "        \n",
    "    chain = conversational_chains.get(selected_topic)\n",
    "    if not chain:\n",
    "        return history + [(user_input, \"Please select a valid topic\")], history\n",
    "        \n",
    "    try:\n",
    "        response = chain({\"question\": user_input, \"chat_history\": history})\n",
    "        history.append((user_input, response['answer']))\n",
    "    except Exception as e:\n",
    "        history.append((user_input, f\"Error: {str(e)}\"))\n",
    "    \n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface(chains):\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"## Multi-Topic RAG-Enhanced Chatbot\")\n",
    "        \n",
    "        selected_topic = gr.Dropdown(\n",
    "            choices=list(chains.keys()),\n",
    "            label=\"Choose a Topic\",\n",
    "            value=list(chains.keys())[0] if chains else None\n",
    "        )\n",
    "        \n",
    "        chatbot = gr.Chatbot()\n",
    "        state = gr.State([])\n",
    "        \n",
    "        with gr.Row():\n",
    "            user_input = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Type your question here...\",\n",
    "                scale=4\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Send\", scale=1)\n",
    "        \n",
    "        submit_click = submit_btn.click(\n",
    "            chat_function,\n",
    "            inputs=[user_input, selected_topic, state],\n",
    "            outputs=[chatbot, state]\n",
    "        )\n",
    "        \n",
    "        user_input.submit(\n",
    "            chat_function,\n",
    "            inputs=[user_input, selected_topic, state],\n",
    "            outputs=[chatbot, state]\n",
    "        )\n",
    "        \n",
    "        submit_click.then(lambda: \"\", None, user_input)\n",
    "        \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 338 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Python Programming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 13 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Programming: Chain initialized\n",
      "Processing Machine Learning_AI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 325 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning_AI: Chain initialized\n",
      "Processing UAE Information...\n",
      "UAE Information: Chain initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/components/chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "Python(3816) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(3823) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://875627278d9993e811.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://875627278d9993e811.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check for API key\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    \n",
    "    # Initialize chains\n",
    "    print(\"Initializing chains...\")\n",
    "    conversational_chains = initialize_chains()\n",
    "    \n",
    "    if not conversational_chains:\n",
    "        print(\"Error: No chains were initialized. Check your data folders and files.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Create and launch Gradio interface\n",
    "    demo = create_gradio_interface(conversational_chains)\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
